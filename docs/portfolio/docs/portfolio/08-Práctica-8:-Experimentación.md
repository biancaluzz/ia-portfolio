---
title: "Entrada 08 ‚Äî Pr√°ctica 8: Experimentaci√≥n"
date: 2025-09-23
---

# üß™ Experimentaci√≥n Sistem√°tica con Redes Neuronales

## Contexto
Experimentaci√≥n exhaustiva con diferentes arquitecturas de redes neuronales, optimizadores y t√©cnicas de regularizaci√≥n para maximizar el accuracy en el dataset CIFAR-10.

## Objetivos
- Realizar experimentaci√≥n sistem√°tica variando arquitecturas de redes neuronales
- Probar diferentes t√©cnicas de regularizaci√≥n (Dropout, BatchNorm, L2)
- Evaluar m√∫ltiples optimizadores y sus hiperpar√°metros
- Lograr el mejor accuracy posible en CIFAR-10 usando MLP

## Actividades (con tiempos estimados)
1. Preparaci√≥n del entorno y dataset CIFAR-10 (10 min)
2. Experimentaci√≥n con arquitecturas (80 min)

## Desarrollo
Se implement√≥ una metodolog√≠a de experimentaci√≥n sistem√°tica basada en la gu√≠a proporcionada, probando combinaciones de:

üß™ Arquitecturas Probadas
- Capas densas: Desde 2 hasta 5 capas
- Funciones de activaci√≥n: ReLU, GELU, Tanh
- Normalizaci√≥n: BatchNormalization entre capas
- Regularizaci√≥n: Dropout (0.0-0.5) y L2 (1e-5 a 1e-4)
- Inicializadores: HeNormal, GlorotUniform

‚öôÔ∏è Optimizadores Evaluados
- Adam: Learning rates 1e-2 a 5e-4
- SGD con momentum: Momentum 0.0-0.95, Nesterov
- RMSprop: Variaci√≥n de learning_rate y rho
- AdamW: Weight decay 1e-5 a 1e-4

‚è±Ô∏è Callbacks Implementados
- EarlyStopping: Paciencia 15 √©pocas, restore_best_weights
- ReduceLROnPlateau: Factor 0.5, paciencia 8 √©pocas
- TensorBoard: Monitoreo en tiempo real

## Evidencias
üéØ Resultados TensorFlow:
  - Training Accuracy: 52.3%
  - Test Accuracy: 49.3%
  - Par√°metros totales: 1,742,474

## Dejo aqu√≠ el enlace al Google Colab donde est√° el an√°lisis completo: [PR√ÅCTICA 8](https://colab.research.google.com/drive/1oCZ_Pl2YhQtMZLrKQeOJMgJXAg-AHo1q?usp=sharing)

## Reflexi√≥n
Lo m√°s desafiante: Ver c√≥mo despu√©s de estar rato exprimentando, no poder superar cierto umbral.

Lo m√°s valioso: La experiencia pr√°ctica de probar sistem√°ticamente diferentes configuraciones y ver su impacto real en el rendimiento. 

Aprendizaje clave: La elecci√≥n de arquitectura debe alinearse con la naturaleza de los datos. A veces, mejor tuning no puede superar las limitaciones de la arquitectura.

Pr√≥ximos pasos: Explorar arquitecturas convolucionales (CNN) que est√°n espec√≠ficamente dise√±adas para im√°genes y deber√≠an superar significativamente estos resultados.
