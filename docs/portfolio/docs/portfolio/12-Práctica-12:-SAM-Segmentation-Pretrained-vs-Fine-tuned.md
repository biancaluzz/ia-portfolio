---
title: "Entrada 12 ‚Äî Pr√°ctica 12: Segmentaci√≥n Sem√°ntica con Segment Anything Model (SAM)"
date: 2025-10-28
---

# üìå Segmentaci√≥n Sem√°ntica con SAM: Explorando Modelos de Segmentaci√≥n Zero-Shot

## Contexto
Implementaci√≥n del modelo Segment Anything (SAM) de Meta para realizar segmentaci√≥n sem√°ntica en im√°genes de √°reas inundadas. El objetivo fue explorar las capacidades de un modelo de segmentaci√≥n zero-shot en un dataset real de detecci√≥n de agua.

## Objetivos
- Configurar el entorno y descargar el dataset de segmentaci√≥n de √°reas inundadas
- Explorar y visualizar el dataset con im√°genes y m√°scaras binarias
- Implementar SAM para generar segmentaciones autom√°ticas
- Evaluar el rendimiento del modelo en tareas de segmentaci√≥n sem√°ntica
- Comparar diferentes estrategias de segmentaci√≥n (autom√°tica vs. con prompts)

## Actividades (con tiempos estimados)
1. Configuraci√≥n del entorno e instalaci√≥n de dependencias (15 min)
2. Descarga y preparaci√≥n del dataset Kaggle (20 min)
3. Exploraci√≥n y an√°lisis del dataset de inundaciones (50 min)
4. Implementaci√≥n de SAM para segmentaci√≥n (30 min)
5. Evaluaci√≥n y visualizaci√≥n de resultados (20 min)

## Desarrollo
Se descarg√≥ el dataset "Flood Area Segmentation" de Kaggle, que contiene 290 im√°genes y sus respectivas m√°scaras binarias. El dataset mostr√≥ una gran variabilidad en tama√±os de imagen (81 tama√±os √∫nicos) con un ratio promedio de p√≠xeles de agua del 42.8%.

Se implement√≥ SAM utilizando tanto el modo autom√°tico (SamAutomaticMaskGenerator) como el modo con prompts (SamPredictor). El modelo demostr√≥ capacidad para segmentar √°reas de agua sin entrenamiento previo espec√≠fico, aprovechando su entrenamiento zero-shot en 11 millones de im√°genes.

## Evidencias
=== DATASET CARGADO ===
Total images: 100
Image shape (primera imagen): (551, 893, 3)
Mask shape (primera m√°scara): (551, 893)

üìä Estad√≠sticas del dataset:
Tama√±os √∫nicos de im√°genes: 81

Water pixel ratio (promedio): 42.80%
Background ratio: 57.20%

<img width="1589" height="1058" alt="image" src="https://github.com/user-attachments/assets/495422e4-2a8f-4c8d-8d5b-12a407228b09" />

=== POINT PROMPT PREDICTION ===
Point: (1, 369)
Confidence score: 0.965

<img width="1589" height="256" alt="image" src="https://github.com/user-attachments/assets/0e23a3a8-282c-4201-82a8-3a6e20f2ccbd" />

=== BOX PROMPT PREDICTION ===
Box: [np.int64(0), np.int64(129), np.int64(928), np.int64(522)]
Confidence score: 0.989

<img width="1589" height="256" alt="image" src="https://github.com/user-attachments/assets/c379a2cf-b437-420e-9067-c3f09ee1d5e6" />

=== M√âTRICAS - POINT PROMPT ===
IoU: 0.8070
Dice: 0.8932
Precision: 0.9681
Recall: 0.8290

=== M√âTRICAS - BOX PROMPT ===
IoU: 0.8016
Dice: 0.8899
Precision: 0.9756
Recall: 0.8180

=== COMPARACI√ìN ===
Box prompt better: False

=== EVALUATING PRETRAINED SAM (Point Prompts) ===
  Processed 20/100 images...
  Processed 40/100 images...
  Processed 60/100 images...
  Processed 80/100 images...
  Processed 100/100 images...

=== PRETRAINED SAM - POINT PROMPTS ===
Mean IoU: 0.5291 ¬± 0.3214
Mean Dice: 0.6220 ¬± 0.3377
Mean Precision: 0.8193
Mean Recall: 0.5885

=== EVALUATING PRETRAINED SAM (Box Prompts) ===
  Processed 20/100 images...
  Processed 40/100 images...
  Processed 60/100 images...
  Processed 80/100 images...
  Processed 100/100 images...

=== PRETRAINED SAM - BOX PROMPTS ===
Mean IoU: 0.7230 ¬± 0.2088
Mean Dice: 0.8156 ¬± 0.1985
Mean Precision: 0.8476
Mean Recall: 0.8106

<img width="1389" height="990" alt="image" src="https://github.com/user-attachments/assets/5b149115-fc59-4e55-99e8-252493fe2ad7" />

=== DATALOADERS CREADOS ===
Train batches: 40
Val batches: 10

Sample batch:
  Images shape: torch.Size([2, 3, 1024, 1024])
  Masks shape: torch.Size([2, 1, 1024, 1024])
  Prompts: 2 items

‚úÖ Loss functions definidas
Test loss: 0.6532

=== FINE-TUNING SETUP ===
Total parameters: 93,735,472
Trainable parameters: 4,058,340
Trainable %: 4.33%

Optimizer: Adam
Learning rate: 0.0001
Scheduler: StepLR (decay every 5 epochs by 0.5)

=== TRAINING COMPLETED ===
Best Val IoU: 0.7495

<img width="1389" height="490" alt="image" src="https://github.com/user-attachments/assets/ef91750c-fa75-4598-bacc-8dd573101eef" />

=== EVALUATING FINE-TUNED SAM ===
  Processed 20/20 images...

=== FINE-TUNED SAM ===
Mean IoU: 0.7335 ¬± 0.1895
Mean Dice: 0.8291 ¬± 0.1585
Mean Precision: 0.9024
Mean Recall: 0.7810

=== COMPARISON ===
Metric          Pretrained      Fine-tuned      Improvement    
------------------------------------------------------------
IOU             0.5291          0.7335          38.63          %
DICE            0.6220          0.8291          33.30          %
PRECISION       0.8193          0.9024          10.15          %
RECALL          0.5885          0.7810          32.70          %

<img width="1389" height="990" alt="image" src="https://github.com/user-attachments/assets/01794615-49c0-4616-9350-76a1fa3e3ecf" />


<img width="1489" height="861" alt="image" src="https://github.com/user-attachments/assets/e7041196-111c-46c8-b900-f6e5c44b60ec" />
=== IMAGE 0 ===
Pretrained: IoU=0.0018, Dice=0.0036
Fine-tuned: IoU=0.7105, Dice=0.8308
Improvement: IoU +0.7087, Dice +0.8271

<img width="1489" height="894" alt="image" src="https://github.com/user-attachments/assets/bfa7e2f0-029b-490f-ac7a-85f5c4535f1b" />
=== IMAGE 5 ===
Pretrained: IoU=0.5862, Dice=0.7391
Fine-tuned: IoU=0.8472, Dice=0.9173
Improvement: IoU +0.2610, Dice +0.1782

<img width="1489" height="848" alt="image" src="https://github.com/user-attachments/assets/01d4811c-a7ec-484e-99f0-fb7d7d28a058" />
=== IMAGE 10 ===
Pretrained: IoU=0.9338, Dice=0.9658
Fine-tuned: IoU=0.9292, Dice=0.9633
Improvement: IoU +-0.0046, Dice +-0.0025

<img width="1489" height="930" alt="image" src="https://github.com/user-attachments/assets/7c761335-8879-45c3-ae14-bc0d3cad2b53" />
=== IMAGE 15 ===
Pretrained: IoU=0.2934, Dice=0.4537
Fine-tuned: IoU=0.3025, Dice=0.4645
Improvement: IoU +0.0091, Dice +0.0108

=== ANALYZING PRETRAINED FAILURES ===
Failure cases: 7

Failure statistics:
  Mean IoU: 0.092
  Mean water region width: 1.00 pixels

<img width="1489" height="436" alt="image" src="https://github.com/user-attachments/assets/5c658d11-ba85-4a2e-9034-80c821d4f321" />

=== ANALYZING FINE-TUNED FAILURES ===
Failure cases: 2

=== FAILURE REDUCTION ===
Pretrained failures: 7
Fine-tuned failures: 2
Reduction: 5 (71.4%)

## Preguntas de Reflexi√≥n

1. ¬øPor qu√© el pretrained SAM puede fallar en detectar agua en im√°genes de inundaciones efectivamente? El pretrained SAM fue entrenado principalmente en objetos con boundaries bien definidos, mientras que el agua de inundaciones tiene caracter√≠sticas diferentes: boundaries difusos, texturas homog√©neas, reflejos que confunden, y variabilidad en apariencia (agua clara vs. turbia).

2. ¬øQu√© componentes de SAM decidiste fine-tunear y por qu√©? ¬øPor qu√© congelamos el image encoder? Solo el mask decoder porque es suficiente para adaptar el modelo a la tarea espec√≠fica manteniendo la eficiencia. Congelamos el image encoder porque: (1) ya captura features visuales gen√©ricas √∫tiles, (2) es computacionalmente costoso reentrenarlo, y (3) el prompt encoder + mask decoder son suficientes para aprender las caracter√≠sticas espec√≠ficas del agua.

3. ¬øC√≥mo se comparan point prompts vs box prompts en este caso de uso de flood segmentation? Point prompts son m√°s eficientes para este caso porque el agua suele formar regiones conectadas donde un punto es suficiente. Box prompts pueden capturar mejor √°reas dispersas pero requieren m√°s precisi√≥n del usuario. En implementaci√≥n autom√°tica, points son m√°s f√°ciles de generar program√°ticamente.

4. ¬øQu√© mejoras espec√≠ficas observaste despu√©s del fine-tuning? (boundaries del agua, false positives, reflections, etc.) Mejora en boundaries difusos del agua, reducci√≥n de falsos positivos en √°reas reflectantes, mejor detecci√≥n de agua turbia o con sedimentos, y mayor robustez frente a variaciones de iluminaci√≥n y sombras.

5. ¬øEste sistema est√° listo para deployment en un sistema de respuesta a desastres? ¬øQu√© falta? No completamente. Faltar√≠a validaci√≥n en datos de diferentes regiones geogr√°ficas, integraci√≥n con fuentes de datos en tiempo real, entre otros.

6. ¬øC√≥mo cambiar√≠a tu approach si tuvieras 10x m√°s datos? ¬øY si tuvieras 10x menos? Entrenar√≠a m√°s componentes (incluyendo parcialmente el image encoder). Con 10x menos usar√≠a transfer learning m√°s agresivo.

7. ¬øQu√© desaf√≠os espec√≠ficos presenta la segmentaci√≥n de agua en inundaciones? Reflejos que parecen agua, sombras que oscurecen el agua real, objetos flotantes que interrumpen la superficie, agua turbia que se confunde con terreno.

## Dejo aqu√≠ el enlace al Google Colab donde est√° el an√°lisis completo: [PR√ÅCTICA 12](https://colab.research.google.com/drive/158e0cgHRZ5u8DKDac9JdWL71Ly5askjU?usp=sharing)

## Reflexi√≥n
Lo m√°s desafiante: Manejar la gran variabilidad de tama√±os en el dataset, requiriendo preprocesamiento adaptativo.

Lo m√°s valioso: Experimentar con un modelo de segmentaci√≥n como SAM y ver su capacidad para generalizar a dominios no vistos durante su entrenamiento.

Aprendizaje clave: Los modelos zero-shot como SAM representan un avance significativo en visi√≥n por computadora, permitiendo aplicaciones espec√≠ficas sin necesidad de fine-tuning excesivo.

Pr√≥ximos pasos: Experimentar con fine-tuning de SAM en el dataset espec√≠fico y comparar el rendimiento con arquitecturas tradicionales de segmentaci√≥n sem√°ntica como U-Net.
