---
title: "Entrada 11 â€” PrÃ¡ctica 11: YOLOv8 Fine-tuning & Tracking"
date: 2025-09-07
---

# ğŸ” DetecciÃ³n de Objetos con YOLO: Del Modelo Base al Fine-Tuning

## Contexto
ImplementaciÃ³n y evaluaciÃ³n del modelo YOLOv8 para detecciÃ³n de objetos en imÃ¡genes de supermercado, explorando las limitaciones de modelos pre-entrenados y el proceso de adaptaciÃ³n mediante fine-tuning para dominios especÃ­ficos.

## Objetivos
- Configurar el entorno para trabajar con YOLOv8 y realizar inferencias bÃ¡sicas
- Evaluar el desempeÃ±o del modelo base (COCO) en imÃ¡genes de productos de supermercado
- Entender las limitaciones de conjuntos de datos genÃ©ricos para casos de uso especÃ­ficos
- Preparar el pipeline para fine-tuning en datasets especializados

## Actividades (con tiempos estimados)
1. Setup del entorno e instalaciÃ³n de dependencias (15 min)
2. Carga y evaluaciÃ³n del modelo YOLOv8 base (20 min)
3. AnÃ¡lisis de inferencias en imÃ¡genes de grocery (40 min)
4. PreparaciÃ³n para fine-tuning y reflexiÃ³n sobre limitaciones (50 min)

## Desarrollo
Se implementÃ³ YOLOv8 nano como modelo base pre-entrenado en el dataset COCO, que contiene 80 clases genÃ©ricas de objetos. Se realizaron inferencias en imÃ¡genes de pasillos de supermercado para evaluar su desempeÃ±o en detecciÃ³n de productos especÃ­ficos.

El modelo base demostrÃ³ capacidades limitadas para identificar productos especÃ­ficos de supermercado, detectando principalmente categorÃ­as genÃ©ricas como "naranjas" y "brÃ³coli" pero sin capacidad para distinguir entre variedades especÃ­ficas o productos empaquetados.

## Evidencias
=== MODELO BASE (COCO) ===
Clases en COCO: 80
Ejemplos de clases: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow']

Clases 'grocery' en COCO: ['apple', 'orange', 'banana', 'carrot', 'bottle', 'cup', 'bowl']
âš ï¸ Nota: COCO tiene clases genÃ©ricas, no productos especÃ­ficos

1. Â¿Por quÃ© elegimos YOLOv8n (nano) en lugar de modelos mÃ¡s grandes?

velocidad, es mÃ¡s rÃ¡pido para pruebas iniciales y consume menos recursos

2. Â¿CuÃ¡ntas clases tiene COCO? Â¿Son suficientes para nuestro caso de uso?

80 clases de objetos comunes (personas, autos, animales, frutas, utensilios, etc.).

3. Â¿QuÃ© significa que COCO tenga "clases genÃ©ricas"?

Significa que las categorÃ­as son muy amplias y no distinguen entre variantes o marcas.

4. Si COCO tiene 'apple', Â¿por quÃ© no sirve para detectar frutas especÃ­ficas en nuestro supermercado?

Porque aprendiÃ³ una sola categorÃ­a â€œappleâ€, no las distintas variedades o presentaciones de manzanas No generaliza a productos reales de gÃ³ndola, donde la iluminaciÃ³n, empaques y etiquetas son diferentes.

=== INFERENCIA CON MODELO BASE ===

image 1/1 /content/grocery_aisle.jpg: 480x640 3 oranges, 2 broccolis, 238.8ms
Speed: 25.6ms preprocess, 238.8ms inference, 12.6ms postprocess per image at shape (1, 3, 480, 640)

<img width="841" height="658" alt="image" src="https://github.com/user-attachments/assets/84ef236f-77fa-43ef-b4a6-d91c6205ee2c" />

ğŸ“Š Objetos detectados: 5
  1. orange: 0.367
  2. orange: 0.297
  3. orange: 0.289
  4. broccoli: 0.235
  5. broccoli: 0.216

Â¿CuÃ¡ntos productos detectÃ³ el modelo base?
DetectÃ³ 4 productos.

Â¿Las detecciones son correctas? Â¿Son especÃ­ficas?
Ninguna detecciÃ³n es correcta. Si, son especificas.

Â¿Por quÃ© crees que el modelo base no funciona bien para productos especÃ­ficos de grocery?
Porque usa COCO, y esta no tiene clases genÃ©ricas.

Â¿QuÃ© clases detecta que NO son Ãºtiles para nuestro caso de uso?
Detecta frutas, y en este caso de uso deberÃ­a detectar verduras.

=== CONFIGURACIÃ“N DEL DATASET ===
NÃºmero de clases: 6
Clases: ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']
Train path: E:\DL\attempt1\attempt3\datasets\Fruits-detection-1\train\images
Val path: E:\DL\attempt1\attempt3\datasets\Fruits-detection-1\valid\images

ğŸ“Š ESTADÃSTICAS FINALES:
  Train images: 0
  Val images: 0
  Total: 0
  Clases: 6

=== EXPLORANDO DATASET ===
Train labels: fruit_detection/Fruits-detection/train/labels
Val labels: fruit_detection/Fruits-detection/valid/labels

=== DISTRIBUCIÃ“N DE CLASES (TRAIN) ===
Total de clases: 6
Clases del dataset: ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']

Apple               : 6070 instancias
Banana              : 2971 instancias
Grape               : 6027 instancias
Orange              : 13938 instancias
Pineapple           : 1372 instancias
Watermelon          : 1683 instancias

<img width="1189" height="590" alt="image" src="https://github.com/user-attachments/assets/b44a257c-a03f-4244-8395-788bcfa7fd74" />

ğŸ“Š ESTADÃSTICAS ADICIONALES:
  Instancias totales: 32061
  Promedio por clase: 5343.5
  Clase mÃ¡s frecuente: Orange (13938 instancias)
  Clase menos frecuente: Pineapple (1372 instancias)

Â¿Las clases estÃ¡n balanceadas? Â¿QuÃ© problemas podrÃ­a causar un desbalance?
SÃ­, estÃ¡n desbalanceadas. Por lo que tiende a detectar mejor los datos de las clases de las que tiene mÃ¡s datos.

Â¿QuÃ© clase tiene mÃ¡s instancias? Â¿Crees que el modelo serÃ¡ mejor detectando esa clase?
Orange tiene mÃ¡s instancias. SÃ­, es mÃ¡s probable que sea mejor.

Â¿La clase con menos instancias podrÃ­a tener mÃ¡s errores? Â¿Por quÃ©?
SÃ­, podrÃ­a tener mÃ¡s errores. Porque al tener mÃ¡s informaciÃ³n de las otras clases, va a ser mejor detectando patrones para las demÃ¡s.

Si tuvieras que agregar mÃ¡s datos, Â¿quÃ© clase priorizarÃ­as y por quÃ©?
SerÃ­a ideal priorizar a las clases con menos instancias: Watermelon, Pineapple y Bananana. De esta manera serÃ¡ mejor detectandolas. Aunque depende del caso de uso, si tu propÃ³sito es detectar solo naranjas, es mejor agregar instancias acÃ¡.

=== VISUALIZANDO EJEMPLOS ===
Encontradas 7108 imÃ¡genes de training
<img width="1545" height="1590" alt="image" src="https://github.com/user-attachments/assets/ae3643ac-41a0-443f-a792-51a7f916755a" />

Â¿Las bounding boxes se ven bien ajustadas a las frutas?
Son bastante precisas, aunque en el caso de la manzana, al estar recortada la imagen, tambiÃ©n aparece recortada la detecciÃ³n de la bounding box. AdemÃ¡s, como se tratan de rectangulos, se adapta a la forma de todos los alimentos.

Â¿Hay frutas que se solapan? Â¿CÃ³mo podrÃ­a afectar esto al modelo?
SÃ­, por ejemplo las uvas. Al solaparse podrÃ­a no interpretar correctamente la forma de la fruta, o al estar recortada no reconocer de quÃ© se trata.

Â¿Las imÃ¡genes tienen buena variedad (tamaÃ±os, Ã¡ngulos, iluminaciÃ³n)?
SÃ­, pareciera que eligieron imÃ¡genes con diferentes Ã¡ngulos y calidad, tambiÃ©n aparecen las frutas cortadas e igual las reconoce

Â¿Notaste alguna anotaciÃ³n incorrecta o faltante?
El bounding box de la manzana no muestra el nombre de la fruta, ya que se ajustÃ³ lo mÃ¡ximo posible para tomar toda la unidad de la manzana. TambiÃ©n en el caso de las uvas, las que se encuentran mÃ¡s alejadas, no son reconocidas por el modelo.

=== RESULTADOS DEL TRAINING ===

<img width="2400" height="1200" alt="image" src="https://github.com/user-attachments/assets/632fe3a8-5adc-45c4-8178-76079f13ef84" />
box_loss (Loss de LocalizaciÃ³n):
1. Â¿QuÃ© mide esta mÃ©trica? Mide el error en la predicciÃ³n de las coordenadas y dimensiones de los bounding boxes (cajas delimitadoras).
2. Â¿CÃ³mo evolucionÃ³ durante el training (aumentÃ³ o disminuyÃ³)? DisminuyÃ³ consistentemente de 1.159 (epoch 1) a 0.7921 (epoch 20)
3. Â¿Por quÃ© queremos que esta mÃ©trica sea baja? Una box_loss baja significa que el modelo estÃ¡ prediciendo bounding boxes mÃ¡s precisos y mejor alineados con los objetos reales.

cls_loss (Loss de ClasificaciÃ³n):
1. Â¿QuÃ© aspecto de la detecciÃ³n mide? Mide el error en la clasificaciÃ³n de los objetos (identificar correctamente si es Apple, Banana, etc.).
2. Si cls_loss es alto, Â¿quÃ© problema tiene el modelo? El modelo tendrÃ­a dificultad para distinguir entre las diferentes clases de frutas.
3. Â¿Observaste mejoras epoch tras epoch? DisminuyÃ³ drÃ¡sticamente de 3.082 a 0.8556.

dfl_loss (Distribution Focal Loss):
1. Â¿Esta mÃ©trica refina la predicciÃ³n de bounding boxes? Si
2. Â¿QuÃ© relaciÃ³n tiene con la precisiÃ³n de las coordenadas? Refina la localizaciÃ³n de bounding boxes mediante distribuciÃ³n de probabilidades. Ayuda a predecir coordenadas mÃ¡s precisas.
3. Â¿DeberÃ­a ser alta o baja al final del training? Definitivamente baja, como se observa al final del training.

Instances:
1. Â¿QuÃ© representa este nÃºmero en cada batch? El nÃºmero de objetos (instancias) detectados en cada batch de entrenamiento.
2. Â¿Por quÃ© varÃ­a entre batches? Diferentes batches tienen diferentes cantidades de objetos. El dataset no estÃ¡ balanceado en tÃ©rminos de objetos por imagen.

GPU_mem:
1. Â¿CuÃ¡nta memoria GPU usÃ³ tu training? Estabilizado en ~4.62GB (desde epoch 10 en adelante)
2. Â¿QuÃ© pasarÃ­a si te quedas sin memoria GPU? Training se detendrÃ­a con error.

Convergencia:
1. Â¿El modelo convergiÃ³ (las losses dejaron de bajar)? SÃ­, pero podrÃ­a mejorar mÃ¡s. Las losses todavÃ­a mostraban tendencia decreciente al final.
2. Â¿CuÃ¡ntos epochs necesitÃ³ para estabilizarse? Aproximadamente 15 epochs para mostrar mejoras consistentes.
3. Si tuvieras mÃ¡s tiempo, Â¿entrenarÃ­as mÃ¡s epochs? SÃ­.


=== CARGANDO MODELO FINE-TUNED ===
Path: /content/runs/detect/fruit_finetuned/weights/last.pt

âœ… Modelo fine-tuned cargado
   Clases: ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']
   Total de clases: 6

ğŸ“Š ComparaciÃ³n:
   Modelo base (COCO): 80 clases (genÃ©ricas)
   Modelo fine-tuned: 6 clases (frutas especÃ­ficas)
   Mejora: Especializado en detecciÃ³n de frutas

=== EVALUACIÃ“N EN VALIDATION SET ===
Ultralytics 8.3.227 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)
Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs
val: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1610.3Â±437.6 MB/s, size: 63.0 KB)
val: Scanning /content/fruit_detection/Fruits-detection/valid/labels.cache... 914 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 914/914 1.4Mit/s 0.0s
val: /content/fruit_detection/Fruits-detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 58/58 4.0it/s 14.3s
                   all        914       3227      0.534      0.365       0.38      0.241
                 Apple        188        557      0.541      0.352      0.384      0.268
                Banana        167        390      0.533       0.39       0.41      0.229
                 Grape        199        809        0.5       0.32      0.325      0.196
                Orange        197       1100      0.555      0.349       0.37      0.233
             Pineapple         77        154      0.535      0.367      0.342      0.208
            Watermelon        107        217      0.541      0.415      0.447       0.31
Speed: 2.5ms preprocess, 4.4ms inference, 0.0ms loss, 1.9ms postprocess per image
Results saved to /content/runs/detect/val

ğŸ“Š MÃ‰TRICAS DEL MODELO FINE-TUNED:
  mAP@0.5:     0.380
  mAP@0.5:0.95: 0.241
  Precision:   0.534
  Recall:      0.365

=== MÃ‰TRICAS POR CLASE ===
Apple               : mAP@0.5 = 0.268
Banana              : mAP@0.5 = 0.229
Grape               : mAP@0.5 = 0.196
Orange              : mAP@0.5 = 0.233
Pineapple           : mAP@0.5 = 0.208
Watermelon          : mAP@0.5 = 0.310

Â¿QuÃ© significa mAP@0.5? Â¿Por quÃ© es importante?
mAP (PrecisiÃ³n Media Promedio) de DetecciÃ³n de Objetos. Porque indica la precisiÃ³n a la hora de identificar la clase y refleja la mejora que ha tenido nuestro modelo en su aprendizaje.

Â¿CuÃ¡l es la diferencia entre mAP@0.5 y mAP@0.5:0.95? Â¿CuÃ¡l es mÃ¡s estricto?
Que mAP@0.5:0.95 es mÃ¡s exigente en la variaciÃ³n. Por lo que es mÃ¡s estricto, ya que requiere una mayor superposiciÃ³n para las posibles coincidencias

Si Precision es alta pero Recall es bajo, Â¿quÃ© problema tiene el modelo?
Muchos falsos negativos.

Si Recall es alto pero Precision es baja, Â¿quÃ© significa?
Muchos falsos positivos.

La precisiÃ³n es cuÃ¡ntos de los casos positivos son verdaderamente positivos (tiene en cuenta TP/(TP+FP)).

El recall es cuÃ¡ntos de los casos negativos son verdadermanete negativos, por lo que la precisiÃ³n se ve penalizada por los falsos positivos (tiene en cuenta TN/(TN+FN))

Â¿QuÃ© clase tiene el mejor mAP? Â¿Coincide con la clase mÃ¡s frecuente del dataset?
No, es watermelon la que tiene mejor mAP, pero es la segunda con menos datos. Parece ser que es mÃ¡s sencillo de detectar el patrÃ³n de watermelon que de otras frutas, por lo que con menos datos es mÃ¡s fÃ¡cil de identificar.

=== COMPARACIÃ“N: BASE vs FINE-TUNED ===
Comparando en 3 imÃ¡genes del validation set


============================================================
Imagen 1/3: 064c52456bdf03af_jpg.rf.f584a76d2d0613f16dc9bbe7477bec5b.jpg
  Modelo Base (COCO):  13 detecciones
  Modelo Fine-tuned:    3 detecciones
  Diferencia:          -10
  Clases (Base):       banana, person, truck
  Clases (Fine-tuned): Banana

<img width="1525" height="788" alt="image" src="https://github.com/user-attachments/assets/08565fc8-a111-4ddc-8094-83420619733d" />

============================================================
Imagen 2/3: aa4b88b1f645dfc1_jpg.rf.8ae7c2c9a7f15ee97dd85b66f026e919.jpg
  Modelo Base (COCO):   1 detecciones
  Modelo Fine-tuned:    2 detecciones
  Diferencia:          +1
  Clases (Base):       banana
  Clases (Fine-tuned): Banana

<img width="1525" height="788" alt="image" src="https://github.com/user-attachments/assets/b4b08045-0c3e-49d3-8dcd-e6d907a323a0" />

============================================================
Imagen 3/3: 0e427c3df6d2dec1_jpg.rf.d78fbe8a085150f7aeb0779d3809f999.jpg
  Modelo Base (COCO):   2 detecciones
  Modelo Fine-tuned:    1 detecciones
  Diferencia:          -1
  Clases (Base):       vase, apple
  Clases (Fine-tuned): Apple

<img width="1525" height="788" alt="image" src="https://github.com/user-attachments/assets/5da6090c-4929-4ed3-a2a5-0c6b442258b7" />

============================================================

Â¿El modelo fine-tuned detectÃ³ mÃ¡s frutas que el base? Â¿Por quÃ©? SÃ­, pero no consistentemente. En la imagen 1, el fine-tuned detectÃ³ 2 frutas mientras el base detectÃ³ 0. Sin embargo, en las otras imÃ¡genes el base detectÃ³ mÃ¡s objetos. Esto porque el modelo fine-tuned estÃ¡ especializado en frutas especÃ­ficas.

Â¿Hubo frutas que el modelo base detectÃ³ pero el fine-tuned no? Â¿CÃ³mo lo explicas? SÃ­, debido a falsos positivos del modelo base o sobre especializaciÃ³n del fine-tuned.

Â¿Las bounding boxes del modelo fine-tuned se ven mÃ¡s ajustadas? SÃ­.

Â¿Notaste diferencias en las confidence scores entre ambos modelos? El fine-tuned tiene confidence scores mÃ¡s altos para frutas especÃ­ficas.

Â¿QuÃ© tipo de errores sigue cometiendo el modelo fine-tuned? Falsos negativos, sensibilidad a variaciones, problemas con frutas superpuestas.

=== ANÃLISIS DE ERRORES ===

=== RESULTADOS COMPARATIVOS ===

Modelo Base (COCO):
  TP: 0, FP: 16, FN: 23
  Precision: 0.000
  Recall:    0.000
  F1-Score:  0.000

Modelo Fine-tuned:
  TP: 4, FP: 2, FN: 19
  Precision: 0.667
  Recall:    0.174
  F1-Score:  0.276

=== MEJORA ===
  Î” Precision: +0.667
  Î” Recall:    +0.174
  Î” F1-Score:  +0.276

<img width="989" height="590" alt="image" src="https://github.com/user-attachments/assets/12e87391-94f1-4b53-89b3-b20b3503cb11" />

Â¿CuÃ¡nto mejorÃ³ el mAP despuÃ©s del fine-tuning?
PrecisiÃ³n: de 0.000 â†’ 0.667 (+0.667)
Recall: de 0.000 â†’ 0.174 (+0.174)
F1-Score: de 0.000 â†’ 0.276 (+0.276)
Â¿QuÃ© clases de productos tienen mejor detecciÃ³n? Â¿CuÃ¡les peor?
MEJOR: Clases con mÃ¡s True Positives (4 detecciones correctas)
PEOR: Clases con muchos False Negatives (19 no detectados)
Â¿Los False Positives disminuyeron? Â¿Y los False Negatives?
False Positives: 16 â†’ 2 (ReducciÃ³n del 87.5%)
False Negatives: 23 â†’ 19 (Mejora del 17%)
Â¿El fine-tuning justificÃ³ el tiempo y esfuerzo? SÃ­, pasÃ³ de ser un modelo inÃºtil a un modelo funcional.

Â¿QuÃ© ajustes harÃ­as para mejorar aÃºn mÃ¡s el modelo? Data augmentation, reducir umbral de confianza, balancear el dataset.


Â¿Por quÃ© distance_threshold=100 pÃ­xeles? Â¿CÃ³mo se relaciona con el tamaÃ±o del frame? En frames de 640x480, 100px representa ~15% del ancho. Permite movimiento normal entre frames sin perder tracking.

Â¿QuÃ© ventaja tiene initialization_delay=2 para reducir false positives? Reduce falsos positivos, requiere 2 detecciones consecutivas.

Si las frutas se mueven muy rÃ¡pido, Â¿deberÃ­as aumentar o disminuir distance_threshold? Aumentar distance_threshold (150-200px) porque los objetos rÃ¡pidos se mueven mÃ¡s entre frames.

Â¿QuÃ© significa que un track "sobreviva" 30 frames sin detecciÃ³n? El track sigue vivo aunque el objeto desaparezde temporalmente. 30 frames = 1 segundo.

Â¿CuÃ¡ndo activarÃ­as filtros de Kalman? Â¿QuÃ© beneficio dan para predicciÃ³n de movimiento? Activar cuando los objetos se mueven con patrones predecibles. Predice la posiciÃ³n futura.


Visualizar Video con Tracking

Â¿Los IDs se mantienen consistentes para cada fruta? No, hay diferentes IDs para mismas frutas

Â¿Hay "ID switches" (una fruta cambia de ID)? SÃ­.

Â¿QuÃ© frutas se detectan mejor? La banana y la naranja

Â¿Hay falsos positivos o negativos? Hay falsos negativos, ya que no detecta todo el tiempo las manzanas o bananas.

ğŸ“Š EstadÃ­sticas generales:
  Total productos trackeados: 13
  DuraciÃ³n promedio: 55.8 frames (1.9s)
  DuraciÃ³n mÃ¡xima: 297 frames (9.9s)
  DuraciÃ³n mÃ­nima: 4 frames (0.1s)

ğŸ“‹ Detalle por producto trackeado:
Track ID     Clase                DuraciÃ³n        Rango Frames        
----------------------------------------------------------------------
Track 1      Orange                217 frames ( 7.2s)    2 â†’ 218 
Track 2      Apple                  14 frames ( 0.5s)    4 â†’ 17  
Track 3      Apple                   4 frames ( 0.1s)    9 â†’ 12  
Track 4      Banana                297 frames ( 9.9s)   46 â†’ 342 
Track 5      Apple                   4 frames ( 0.1s)   80 â†’ 83  
Track 6      Orange                 18 frames ( 0.6s)   82 â†’ 99  
Track 7      Apple                   8 frames ( 0.3s)   87 â†’ 94  
Track 8      Apple                  12 frames ( 0.4s)  173 â†’ 184 
Track 9      Apple                  44 frames ( 1.5s)  188 â†’ 231 
Track 10     Banana                 16 frames ( 0.5s)  257 â†’ 272 
Track 11     Orange                 10 frames ( 0.3s)  262 â†’ 271 
Track 12     Orange                 30 frames ( 1.0s)  286 â†’ 315 
Track 13     Orange                 51 frames ( 1.7s)  292 â†’ 342 

ğŸ“¦ Productos por clase:
  Apple               :   6 tracks
  Orange              :   5 tracks
  Banana              :   2 tracks

<img width="1590" height="1190" alt="image" src="https://github.com/user-attachments/assets/00f4afb9-84d4-4eea-bc48-7d6b198c7291" />

âš¡ MÃ©tricas de calidad del tracking:
  Tracks cortos (<1s):  8 (61.5%)
  Tracks largos (>3s):  2 (15.4%)
  Tracks totales:       13

Â¿CuÃ¡ntos productos diferentes se trackearon en el video? 13 IDs
Apple: 6
Orange: 5
Banana: 2
Â¿Los IDs se mantuvieron consistentes o hubo switches? Mismo producto cambia de ID mÃºltiples veces.

Â¿QuÃ© productos tienen tracking mÃ¡s estable? Â¿CuÃ¡les menos?

MÃS ESTABLE: Banana (Track 4 - 9.9s) y Orange (Track 1 - 7.2s)
MENOS ESTABLE: Apple (mÃºltiples tracks cortos <0.5s)
Â¿CÃ³mo podrÃ­as mejorar la estabilidad del tracking?
Reducir distance_treshold para mas precisiÃ³n.
Aumentar hit_counter_max para mayor tolerancia a oclusiones.
Â¿Este sistema serÃ­a Ãºtil para tu aplicaciÃ³n de retail? Â¿QuÃ© ajustes harÃ­as? Es Ãºtil para conteo de productos, para ello es necesarios reducir los ID switches, y mejorar la detecciÃ³n de clases problemÃ¡ticas (Apple).

## ğŸ¯ ReflexiÃ³n Final: Integrando Todo el Assignment

Sobre el Modelo:

1. Â¿CuÃ¡l fue la mejora mÃ¡s significativa del fine-tuning? (mAP, FPs, FNs): PrecisiÃ³n de 0% â†’ 67% (el modelo pasÃ³ de inÃºtil a funcional)

2. Â¿El modelo base (COCO) era completamente inÃºtil o tenÃ­a algo de valor? COCO proporcionÃ³ features generales que aceleraron el fine-tuning, demostrando que el transfer learning es eficiente.

3. Si tuvieras que hacer fine-tuning para otro dominio (ej: piezas industriales), Â¿quÃ© aprenderÃ­as de esta experiencia?  La estrategia de fine-tuning progresivo funciona.


Sobre los Datos:

1. Â¿8,479 imÃ¡genes es mucho o poco para fine-tuning? Â¿Por quÃ© funcionÃ³ usar solo 25%? Suficiente para fine-tuning gracias al transfer learning. El 25% funcionÃ³ porque el modelo ya tenÃ­a features bÃ¡sicas de COCO.

2. Â¿La calidad de las anotaciones afectÃ³ los resultados? Â¿CÃ³mo lo sabes? AfectÃ³ directamente el recall bajo - algunas frutas no se detectaban por anotaciones incompletas o difÃ­ciles.

3. Si pudieras agregar 1,000 imÃ¡genes mÃ¡s, Â¿de quÃ© tipo serÃ­an? PriorizarÃ­a casos difÃ­ciles: oclusiones, iluminaciÃ³n variable, Ã¡ngulos atÃ­picos y la clase con peor performance.


Sobre el Tracking:

1. Â¿QuÃ© fue mÃ¡s importante para un buen tracking: el modelo o los parÃ¡metros del tracker? Primero el modelo (sin buenas detecciones no hay tracking), luego los parÃ¡metros del tracker.

2. Â¿Norfair (IoU-based) es suficiente o necesitas algo mÃ¡s sofisticado como DeepSORT? Norfair es suficiente para casos simples, pero DeepSORT serÃ­a mejor para objetos similares usando apariencia visual.

3. Â¿Los filtros de Kalman mejoraron la estabilidad del tracking? Â¿En quÃ© situaciones? Mejoraron con movimiento predecible (cinta transportadora), menos con objetos estÃ¡ticos o movimiento errÃ¡tico.

4. Â¿En quÃ© escenarios fallarÃ­a este sistema de tracking? Oclusiones prolongadas, cambios bruscos de movimiento, objetos muy similares visualmente.


Sobre el Deployment:

1. Â¿Este sistema podrÃ­a correr en tiempo real? Â¿QuÃ© FPS necesitarÃ­as?
Â¿QuÃ© optimizaciones harÃ­as para producciÃ³n? (modelo, cÃ³digo, hardware) SÃ­, con optimizaciones (YOLOv8s, TensorRT).

2. Â¿CÃ³mo manejarÃ­as casos extremos? (oclusiones, iluminaciÃ³n, Ã¡ngulos raros) Data augmentation especÃ­fica, mÃºltiples cÃ¡maras, reglas de negocio para casos ambiguos.


Trade-offs y Decisiones:

1. Identifica 3 trade-offs clave que encontraste (ej: speed vs accuracy, epochs vs tiempo)
- Accuracy vs Speed: Modelo mÃ¡s preciso = mÃ¡s lento
- Recall vs Precision: MÃ¡s detecciones = mÃ¡s falsos positivos
- Training time vs Performance: MÃ¡s epochs = mejor modelo pero mÃ¡s costo

2. Â¿CuÃ¡l fue la decisiÃ³n mÃ¡s importante que tomaste en los hyperparÃ¡metros? Learning rate 1e-5 para fine-tuning

3. Si tuvieras que explicar este proyecto a un stakeholder no-tÃ©cnico, Â¿quÃ© 3 puntos destacarÃ­as?
- Automatiza conteo y tracking de productos
- Reduce errores humanos en inventario
- Escalable para mÃºltiples tiendas con bajo costo marginal


## Dejo aquÃ­ el enlace al Google Colab donde estÃ¡ el anÃ¡lisis completo: [PRÃCTICA 11](https://colab.research.google.com/drive/1tdq81F_iBwAnhh5AKkgkjX96GEBgCeeu?usp=sharing)

## ReflexiÃ³n
Lo mÃ¡s desafiante: Entender las limitaciones prÃ¡cticas de modelos pre-entrenados en dominios especÃ­ficos y la necesidad de adaptaciÃ³n mediante fine-tuning.

Lo mÃ¡s valioso: La experiencia prÃ¡ctica de evaluar un modelo en escenarios reales y visualizar directamente sus fortalezas y debilidades.

Aprendizaje clave: Los modelos genÃ©ricos requieren especializaciÃ³n para aplicaciones especÃ­ficas mediante transfer learning y datasets domain-specific.

PrÃ³ximos pasos: Implementar fine-tuning con dataset especializado de productos de supermercado y evaluar mejora en precisiÃ³n para el dominio especÃ­fico.
